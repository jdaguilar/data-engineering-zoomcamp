{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 Homework \n",
    "\n",
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "For this homework we will be using the FHVHV 2021-06 data found here. [FHVHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?\n",
    "- 3.3.2\n",
    "- 2.1.4\n",
    "- 1.2.3\n",
    "- 5.4\n",
    "</br></br>\n",
    "\n",
    "R:/ 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 06:40:57 WARN Utils: Your hostname, DESKTOP-LRRD21B resolves to a loopback address: 127.0.1.1; using 172.17.195.122 instead (on interface eth0)\n",
      "23/03/08 06:40:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/08 06:41:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/08 06:41:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Spark context Web UI available at http://172.17.195.122:4041\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1678275674103).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_352)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\u001b[35m\n",
      "scala> \u001b[0m\n",
      "\u001b[35m\n",
      "scala> \u001b[0m"
     ]
    }
   ],
   "source": [
    "!spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 06:55:07 WARN Utils: Your hostname, DESKTOP-LRRD21B resolves to a loopback address: 127.0.1.1; using 172.17.195.122 instead (on interface eth0)\n",
      "23/03/08 06:55:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 06:55:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "**HVFHW June 2021**\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons.</br> \n",
    "We will use this dataset for all the remaining questions.</br>\n",
    "Repartition it to 12 partitions and save it to parquet.</br>\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.</br>\n",
    "\n",
    "\n",
    "- 2MB\n",
    "- 24MB\n",
    "- 100MB\n",
    "- 250MB\n",
    "\n",
    "R:/ 24 MB\n",
    "</br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 07:04:46 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 96.70% for 7 writers\n",
      "23/03/08 07:04:46 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 84.61% for 8 writers\n",
      "23/03/08 07:04:46 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 75.21% for 9 writers\n",
      "23/03/08 07:04:46 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 67.69% for 10 writers\n",
      "23/03/08 07:04:46 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 61.53% for 11 writers\n",
      "23/03/08 07:04:46 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 56.41% for 12 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/08 07:04:49 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 61.53% for 11 writers\n",
      "23/03/08 07:04:49 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 67.69% for 10 writers\n",
      "23/03/08 07:04:49 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 75.21% for 9 writers\n",
      "23/03/08 07:04:49 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 84.61% for 8 writers\n",
      "23/03/08 07:04:49 WARN MemoryManager: Total allocation exceeds 95.00% (908,486,235 bytes) of heap memory\n",
      "Scaling row group sizes to 96.70% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-06.csv')\n",
    "\n",
    "df = df.repartition(12)\n",
    "\n",
    "df.repartition(12).write.mode(\"overwrite\").parquet('data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "\n",
    "**Count records**  \n",
    "\n",
    "How many taxi trips were there on June 15?</br></br>\n",
    "Consider only trips that started on June 15.</br>\n",
    "\n",
    "- 308,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 50,982\n",
    "\n",
    "R: 452,470\n",
    "</br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read \\\n",
    "    .option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .parquet('data/pq/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02510|2021-06-01 16:24:08|2021-06-01 17:25:06|         129|          69|      N|                  null|\n",
      "|              B02510|2021-06-01 12:48:33|2021-06-01 13:03:00|         229|         100|      N|                  null|\n",
      "|              B02879|2021-06-01 07:20:27|2021-06-01 08:10:56|          68|         265|      N|                B02879|\n",
      "|              B02889|2021-06-02 10:00:42|2021-06-02 10:09:50|         164|          79|      N|                B02889|\n",
      "|              B02872|2021-06-03 11:38:01|2021-06-03 11:59:39|         162|          79|      N|                B02872|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2021-06-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day**  \n",
    "\n",
    "Now calculate the duration for each trip.</br>\n",
    "How long was the longest trip in Hours?</br>\n",
    "\n",
    "- 66.87 Hours\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours\n",
    "</br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.createOrReplaceTempView ('fhvhv_2021_06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|pickup_date|          duration|\n",
      "+-----------+------------------+\n",
      "| 2021-06-25| 4012.733333333333|\n",
      "| 2021-06-22|1532.9833333333333|\n",
      "| 2021-06-27|           1198.85|\n",
      "| 2021-06-26|1091.8333333333333|\n",
      "| 2021-06-23| 988.0166666666667|\n",
      "| 2021-06-24| 834.5833333333334|\n",
      "| 2021-06-04|             700.2|\n",
      "| 2021-06-20| 659.0666666666667|\n",
      "| 2021-06-01|            616.05|\n",
      "| 2021-06-28| 597.9833333333333|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        to_date(pickup_datetime) AS pickup_date,\n",
    "        MAX((CAST(dropoff_datetime AS LONG) - CAST(pickup_datetime AS LONG)) / 60) AS duration\n",
    "    FROM \n",
    "        fhvhv_2021_06\n",
    "    GROUP BY\n",
    "        pickup_date\n",
    "    ORDER BY\n",
    "        duration DESC\n",
    "    LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: \n",
    "\n",
    "**User Interface**\n",
    "\n",
    " Spark’s User Interface which shows application's dashboard runs on which local port?</br>\n",
    "\n",
    "- 80\n",
    "- 443\n",
    "- 4040\n",
    "- 8080\n",
    "\n",
    "R: 4040\n",
    "</br></br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Most frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)</br>\n",
    "\n",
    "Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?</br>\n",
    "\n",
    "- East Chelsea\n",
    "- Astoria\n",
    "- Union Sq\n",
    "- Crown Heights North\n",
    "\n",
    "R: Crown Heights North\n",
    "</br></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_df = spark.read \\\n",
    "    .option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02510|2021-06-01 16:24:08|2021-06-01 17:25:06|         129|          69|      N|                  null|\n",
      "|              B02510|2021-06-01 12:48:33|2021-06-01 13:03:00|         229|         100|      N|                  null|\n",
      "|              B02879|2021-06-01 07:20:27|2021-06-01 08:10:56|          68|         265|      N|                B02879|\n",
      "|              B02889|2021-06-02 10:00:42|2021-06-02 10:09:50|         164|          79|      N|                B02889|\n",
      "|              B02872|2021-06-03 11:38:01|2021-06-03 11:59:39|         162|          79|      N|                B02872|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+---------+--------------------+------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|LocationID|  Borough|                Zone|service_zone|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+---------+--------------------+------------+\n",
      "|              B02510|2021-06-01 16:24:08|2021-06-01 17:25:06|         129|          69|      N|                  null|       129|   Queens|     Jackson Heights|   Boro Zone|\n",
      "|              B02510|2021-06-01 12:48:33|2021-06-01 13:03:00|         229|         100|      N|                  null|       229|Manhattan|Sutton Place/Turt...| Yellow Zone|\n",
      "|              B02879|2021-06-01 07:20:27|2021-06-01 08:10:56|          68|         265|      N|                B02879|        68|Manhattan|        East Chelsea| Yellow Zone|\n",
      "|              B02889|2021-06-02 10:00:42|2021-06-02 10:09:50|         164|          79|      N|                B02889|       164|Manhattan|       Midtown South| Yellow Zone|\n",
      "|              B02872|2021-06-03 11:38:01|2021-06-03 11:59:39|         162|          79|      N|                B02872|       162|Manhattan|        Midtown East| Yellow Zone|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+----------+---------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = df2.join(\n",
    "    zones_df,\n",
    "    df2.PULocationID ==  zones_df.LocationID,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Zone| count|\n",
      "+--------------------+------+\n",
      "| Crown Heights North|231279|\n",
      "|        East Village|221244|\n",
      "|         JFK Airport|188867|\n",
      "|      Bushwick South|187929|\n",
      "|       East New York|186780|\n",
      "|TriBeCa/Civic Center|164344|\n",
      "|   LaGuardia Airport|161596|\n",
      "|            Union Sq|158937|\n",
      "|        West Village|154698|\n",
      "|             Astoria|152493|\n",
      "|     Lower East Side|151020|\n",
      "|        East Chelsea|147673|\n",
      "|Central Harlem North|146402|\n",
      "|Williamsburg (Nor...|143683|\n",
      "|          Park Slope|143594|\n",
      "|  Stuyvesant Heights|141427|\n",
      "|        Clinton East|139611|\n",
      "|West Chelsea/Huds...|139431|\n",
      "|             Bedford|138428|\n",
      "|         Murray Hill|137879|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.groupBy('Zone').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the solutions\n",
    "\n",
    "* Form for submitting: https://forms.gle/EcSvDs6vp64gcGuD8\n",
    "* You can submit your homework multiple times. In this case, only the last submission will be used. \n",
    "\n",
    "Deadline: 06 March (Monday), 22:00 CET\n",
    "\n",
    "\n",
    "## Solution\n",
    "\n",
    "We will publish the solution here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
